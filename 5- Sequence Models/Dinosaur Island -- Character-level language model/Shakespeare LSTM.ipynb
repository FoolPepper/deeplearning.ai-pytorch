{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch as pt\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import re\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "pt.set_printoptions(linewidth=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = pt.device(\"cuda:0\" if pt.cuda.is_available() else \"cpu\")\n",
    "hidden_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class ShakespeareDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        with open('shakespeare.txt') as f:\n",
    "            content = f.read().lower()\n",
    "            self.vocab = sorted(set(content))\n",
    "            self.vocab_size = len(self.vocab)\n",
    "            self.poems = [poem for poem in re.split('\\s{2,}', content) if len(poem) > 100]\n",
    "        self.ch_to_idx = {c:i for i, c in enumerate(self.vocab)}\n",
    "        self.idx_to_ch = {i:c for i, c in enumerate(self.vocab)}\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        poem = self.poems[index]\n",
    "        x_str = poem\n",
    "        y_str = poem[1:] + '\\n'\n",
    "        x = pt.zeros([len(x_str), self.vocab_size], dtype=pt.float)\n",
    "        y = pt.empty(len(x_str), dtype=pt.long)\n",
    "        for i, (x_ch, y_ch) in enumerate(zip(x_str, y_str)):\n",
    "            x[i][self.ch_to_idx[x_ch]] = 1\n",
    "            y[i] = self.ch_to_idx[y_ch]\n",
    "        \n",
    "        return x, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.poems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_ds = ShakespeareDataset()\n",
    "trn_dl = DataLoader(trn_ds, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.l1 = nn.ModuleList(self.make_lstm_layer(input_size, hidden_size, hidden_size))\n",
    "        self.l2 = nn.ModuleList(self.make_lstm_layer(hidden_size, hidden_size, output_size))\n",
    "        \n",
    "    def forward(self, c_prev_1, h_prev_1, c_prev_2, h_prev_2, x):\n",
    "        c_1, h_1, output_1 = self.forward_layer(self.l1, c_prev_1, h_prev_1, x)\n",
    "        c_2, h_2, y = self.forward_layer(self.l2, c_prev_2, h_prev_2, output_1)\n",
    "        return c_1, h_1, c_2, h_2, y\n",
    "        \n",
    "    def forward_layer(self, l, c_prev, h_prev, x):\n",
    "        combined = pt.cat([x, h_prev], 1)\n",
    "        f = pt.sigmoid(l[0](combined))\n",
    "        u = pt.sigmoid(l[1](combined))\n",
    "        c_tilde = pt.tanh(l[2](combined))\n",
    "        c = f*c_prev + u*c_tilde\n",
    "        o = pt.sigmoid(l[3](combined))\n",
    "        h = o*pt.tanh(c)\n",
    "        output = l[4](h)\n",
    "        \n",
    "        return c, h, output\n",
    "        \n",
    "    def make_lstm_layer(self, input_size, hidden_size, output_size):\n",
    "        linear_f = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        linear_u = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        linear_c = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        linear_o = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        i2o = nn.Linear(hidden_size, output_size)\n",
    "        return [linear_f, linear_u, linear_c, linear_o, i2o]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM(trn_ds.vocab_size, hidden_size, trn_ds.vocab_size).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def print_sample(sample_idxs):\n",
    "    [print(trn_ds.idx_to_ch[x], end='') for x in sample_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def sample(model):\n",
    "    model.eval()\n",
    "    with pt.no_grad():\n",
    "        c_prev_1 = pt.zeros([1, hidden_size], dtype=pt.float, device=device)\n",
    "        h_prev_1 = pt.zeros_like(c_prev_1)\n",
    "        c_prev_2 = pt.zeros_like(c_prev_1)\n",
    "        h_prev_2 = pt.zeros_like(c_prev_1)\n",
    "        \n",
    "        idx = random.randint(1, trn_ds.vocab_size-1)\n",
    "        x = c_prev_1.new_zeros([1, trn_ds.vocab_size])\n",
    "        x[0, idx] = 1\n",
    "        sampled_indexes = [idx]\n",
    "        n_chars = 1\n",
    "        newline_char_idx = trn_ds.ch_to_idx['\\n']\n",
    "        num_lines = 0\n",
    "        while n_chars != 1000 and num_lines != 5:\n",
    "            c_prev_1, h_prev_1, c_prev_2, h_prev_2, y_pred = model(c_prev_1, h_prev_1, c_prev_2, h_prev_2, x)\n",
    "            \n",
    "            np.random.seed(np.random.randint(1, 5000))\n",
    "            idx = np.random.choice(np.arange(trn_ds.vocab_size), p=pt.softmax(y_pred, 1).cpu().numpy().ravel())\n",
    "            sampled_indexes.append(idx)\n",
    "            x = pt.zeros_like(x)\n",
    "            x[0, idx] = 1\n",
    "            \n",
    "            n_chars += 1\n",
    "            \n",
    "            if idx == newline_char_idx:\n",
    "                num_lines += 1\n",
    "            \n",
    "        if n_chars == 50:\n",
    "            sampled_indexes.append(newline_char_idx)\n",
    "                \n",
    "    model.train()\n",
    "    return sampled_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loss_fn, optimizer):\n",
    "    for poem_num, (x, y) in enumerate(trn_dl):\n",
    "        model.train()\n",
    "        loss = 0\n",
    "        optimizer.zero_grad()\n",
    "        c_prev_1 = pt.zeros([1, hidden_size], dtype=pt.float, device=device)\n",
    "        h_prev_1 = pt.zeros_like(c_prev_1)\n",
    "        c_prev_2 = pt.zeros_like(c_prev_1)\n",
    "        h_prev_2 = pt.zeros_like(c_prev_1)\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        for i in range(x.shape[1]):\n",
    "            c_prev_1, h_prev_1, c_prev_2, h_prev_2, y_pred = model(c_prev_1, h_prev_1, c_prev_2, h_prev_2, x[:, i])\n",
    "            loss += loss_fn(y_pred, y[:, i])\n",
    "        print('poem num:', poem_num)\n",
    "            \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print_sample(sample(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def train(model, loss_fn, optimizer, epochs=1):\n",
    "    for e in range(1, epochs+1):\n",
    "        print(f'Epoch:{e}')\n",
    "        train_one_epoch(model, loss_fn, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1\n",
      "poem num: 0\n",
      "v??lfwko:hcsxoe(o\n",
      "jsl .i:m:mx!f uwgyfffzaq\n",
      "mf)\n",
      "s-w!:r:mwq\n",
      "wj!drav )rh\n",
      "poem num: 1\n",
      "fg,sbc t exmv!strg.ppeysforu\n",
      "ef ?s r?u !rg\n",
      "g)sbc t exmv!strg.ppeysforu\n",
      "ef ?s r?u !rg\n",
      "g)sbc t exmv!strg.ppeysforu\n",
      "poem num: 2\n",
      "kef ?s r?u  rg\n",
      "g(sac t exmv strf-ppeysfort\n",
      "ee ?s r?u  rg\n",
      "g(sac t exmv strf-ppeysfort\n",
      "ee ?s r?u  rg\n",
      "poem num: 3\n",
      "qg)sac t exmv strf-ppeysfort\n",
      "ee ?s r?u  rg\n",
      "f(sac t exmv strf-ppeysfort\n",
      "ee ?s r?u  rg\n",
      "f(sac t exmv strf-ppeysfort\n",
      "poem num: 4\n",
      "eee ?s r?t  rf\n",
      "e sac t ewlt stse)ppeyseort ee ;s r?t  re\n",
      "e sac t ewlt stse)ppeyseort ee ;s r?t  re\n",
      "e sac t ewlt stse)ppeyseort ee ;s r?t  re\n",
      "e sac t ewlt stse)ppeyseort ee ;s r?t  re\n",
      "poem num: 5\n",
      "sh(tbc t ewmt stsg,ppeytfost\n",
      "ef as sat  sh\n",
      "g tac t ewmt stsg,ppeytfost\n",
      "ef as sat  sh\n",
      "g tac t ewmt stsg,ppeytfost\n",
      "poem num: 6\n",
      "neh ds set )si\n",
      "i?tee t hwnt(stsibrrhysiost\n",
      "gi es set )si\n",
      "i?tee t hwnt(stsibrrhysiost\n",
      "gi es set )si\n",
      "poem num: 7\n",
      "kf r   o (ues nona ll)woahmr  a  n m r  ma a o   o  ues nona ll)woahmr  a  n m r  ma a o   o  ues nona ll)woahmr  a  n m r  ma a o   o  ues nona ll)woahmr  a  n m r  ma a o   o  ues nona ll)woahmr  a  n m r  ma a o   o  ues nona ll)woahmr  a  n m r  ma a o   o  ues nona ll)woahmr  a  n m r  ma a o   o  ues nona ll)woahmr  a  n m r  ma a o   o  ues nona ll)woahmr  a  n m r  ma a o   o  ues nona ll)woahmr  a  n m r  ma a o   o  ues nona ll)woahmr  a  n m r  ma a o   o  ues nona ll)woahmr  a  n m r  ma a o   o  ues nona ll)woahmr  a  n m r  ma a o   o  ues nona ll)woahmr  a  n m r  ma a o   o  ues nona ll)woahmr  a  n m r  ma a o   o  ues nona ll)woahmr  a  n m r  ma a o   o  ues nona ll)woahmr  a  n m r  ma a o   o  ues nona ll)woahmr  a  n m r  ma a o   o  ues nona ll)woahmr  a  n m r  ma a o   o  ues nona ll)woahmr  a  n m r  ma a o   o  ues nona ll)woahmr  a  n m r  ma a o   o  ues nona ll)woahmr  a  n m r  ma a o   o  ues nona ll)woahmr  a  n m r  ma a o   o  ues nona ll)woahmr  a  npoem num: 8\n",
      ".'seu!ask\n",
      "jbtef't)hwouatttjdrrhytirst\n",
      "hi'et(seu!ask\n",
      "jbtef't)hwouatttjdrrhytirst\n",
      "hi'et(seu!ask\n",
      "poem num: 9\n",
      "dh.tde t exnv(stsh?ppfyshort\n",
      "eg bs rbu (rh\n",
      "h-tde t exnv(stsh?ppfyshort\n",
      "eg bs rbu (rh\n",
      "h-tde t exnv(stsh?ppfyshort\n",
      "poem num: 10\n",
      "peg bs rbt  rh\n",
      "h,sde t ewnu stsh;ppeysgort\n",
      "eg bs rbt  rh\n",
      "h,sde t ewnu stsh;ppeysgort\n",
      "eg bs rbt  rh\n",
      "poem num: 11\n",
      "dh,sde t ewnu stsh?ppeyshort\n",
      "eg bs rct !rh\n",
      "h,sde t ewnu stsh?ppeyshort\n",
      "eg bs rct !rh\n",
      "h,sde t ewnu stsh?ppeyshort\n",
      "poem num: 12\n",
      "?eg bs rbt  rh\n",
      "h(sde t ewnt stsh;ppeysgort\n",
      "ef bs rbt  rh\n",
      "h(sde t ewnt stsh;ppeysgort\n",
      "ef bs rbt  rh\n",
      "poem num: 13\n",
      ")h-tee t evnt stsharrfyshost\n",
      "eh ds sdt  sh\n",
      "h,tee t evnt stsharrfyshost\n",
      "eh ds sdt  sh\n",
      "h,tee t evnt stsharrfyshost\n",
      "poem num: 14\n",
      "(eg bs sbt  sh\n",
      "h,sde t evnt stshaqqeyshost\n",
      "eg bs sbt  sh\n",
      "h,sde t evnt stshaqqeyshost\n",
      "eg bs sbt  sh\n",
      "poem num: 15\n",
      "vh,see t evnt stshaqqfxshost\n",
      "eh cs sct  sh\n",
      "h)see s evnt stshaqqfxshost\n",
      "eh cs sct  sh\n",
      "h)see s evnt stshaqqfxshost\n",
      "poem num: 16\n",
      "veh ds rdt  rh\n",
      "h,see s evnt ssshappfxshort\n",
      "eh ds rdt  rh\n",
      "h,see s evnt ssshappfxshort\n",
      "eh ds rdt  rh\n",
      "poem num: 17\n",
      ".h,tee t fvnt stsharrgxshost\n",
      "eh ds sdt  si\n",
      "h,tee t fvnt stsharrgxshost\n",
      "eh ds sdt  si\n",
      "h,tee t fvnt stsharrgxshost\n",
      "poem num: 18\n",
      "'eg bs rbt  rh\n",
      "h sde t evnt stsh.ooeysgort\n",
      "eg as rbt  rh\n",
      "h sde t evnt stsh.ooeysgort\n",
      "eg as rbt  rh\n",
      "poem num: 19\n",
      "gh,tee t evnt stshappfyshost\n",
      "eh ds sdt  sh\n",
      "h,see t evnt stshappfyshost\n",
      "eh ds sdt  sh\n",
      "h,see t evnt stshappfyshost\n",
      "poem num: 20\n",
      "geh bs rbt  rh\n",
      "h sde s evnt ssrh-ooeysgort\n",
      "eg as rbt  rh\n",
      "h sde s evnt ssrh-ooeysgort\n",
      "eg as rbt  rh\n",
      "poem num: 21\n",
      "bh,see s fvnt ssrhapofyshort\n",
      "eh ds rdt  ri\n",
      "i)see s fvnt ssrhapofyshort\n",
      "eh ds rdt  ri\n",
      "i)see s fvnt ssrhapofyshort\n",
      "poem num: 22\n",
      "reh es ret  rh\n",
      "h,see t fvnt strhaqqgyshort\n",
      "eh es ret  rh\n",
      "h,see t fvnt strhaqqgyshort\n",
      "eh es ret  rh\n",
      "poem num: 23\n",
      "ih,see s evnt ssrhappfyshort\n",
      "eh ds rdt  rh\n",
      "h)see s evnt ssrhappfyshort\n",
      "eh ds rdt  rh\n",
      "h)see s evnt ssrhappfyshort\n",
      "poem num: 24\n",
      ".eh cs rdt  ri\n",
      "h see s fvnt strhappgxshort\n",
      "eh cs rdt  ri\n",
      "h see s fvnt strhappgxshort\n",
      "eh cs rdt  ri\n",
      "poem num: 25\n",
      "-h.see t gvnt stsibrrhxshort\n",
      "fh es set  si\n",
      "i:see t gvnt stsibrrhxshort\n",
      "fh es set  si\n",
      "i:see t gvnt stsibrrhxshort\n",
      "poem num: 26\n",
      "weg ds sdt  sh\n",
      "h see s evnt ssshaqqewsgort\n",
      "ef ds sdt  sh\n",
      "h see s evnt ssshaqqewsgort\n",
      "ef ds sdt  sh\n",
      "poem num: 27\n",
      "rh(see s evnt ssshappexshort\n",
      "eg cs rdt  rh\n",
      "h sde s evnt ssshappexshort\n",
      "eg cs rdt  rh\n",
      "h sde s evnt ssshappexshort\n",
      "poem num: 28\n",
      "hhi es set asl\n",
      "latgh t ivou:stslerriytlrst\n",
      "il es set asl\n",
      "latgh t ivou:stslerriytlrst\n",
      "il es set asl\n",
      "poem num: 29\n",
      "!i,see t hvot stsiarrhxsiost\n",
      "hi es set  si\n",
      "i,see t hvot stsiarrhxsiost\n",
      "hi es set  si\n",
      "i,see t hvot stsiarrhxsiost\n",
      "poem num: 30\n",
      "veg ds rdt  rh\n",
      "h'see s evnt rsrhaooexsgort\n",
      "eg ds rdt  rh\n",
      "h'see s evnt rsrhaooexsgort\n",
      "eg ds rdt  rh\n",
      "poem num: 31\n",
      "vi.tee t hvnt stsibrrhysiost\n",
      "hi es set  si\n",
      "iatef t hvnt stsibrrhysiost\n",
      "hi es set  si\n",
      "iatef t hvnt stsibrrhysiost\n",
      "poem num: 32\n",
      "aee ?r p:t  pf\n",
      "f sab s evlt rsre ooexrenps\n",
      "ee ,r p.t  pf\n",
      "f sab s evlt rsre ooexrenps\n",
      "ee ,r p.t  pf\n",
      "poem num: 33\n",
      "xiatee t hvntastsidrrhytiost\n",
      "hi es set asi\n",
      "iatef t hvntastsidrrhytiost\n",
      "hi es set asi\n",
      "iatef t hvntastsidrrhytiost\n",
      "poem num: 34\n",
      "-ef as rat  rh\n",
      "h sac s evmt rsrg ooeysfort\n",
      "ef ar rat  rh\n",
      "h sac s evmt rsrg ooeysfort\n",
      "ef ar rat  rh\n",
      "poem num: 35\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-d43ce001251c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-33-7555a68a9bd6>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, loss_fn, optimizer, epochs)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Epoch:{e}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-32-2d21fdefc66b>\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'poem num:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpoem_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mprint_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pytorch-0.4/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pytorch-0.4/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(model, loss_fn, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_ds.lines[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-0.4",
   "language": "python",
   "name": "pytorch-0.4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
